\section{Experimental Results}
\label{sec:exp}

In this section, we first introduce the experimental setup, then compare the performance of DCSM with previous systems in terms of throughput and response time, and finally evaluate the effectiveness of our proposed optimizations through ablation studies.
%______________________________________________________________________________________________________________________________________________________%
\subsection{Experimental Setup}
\label{sec:exp_setup}

% Table generated by Excel2LaTeX from sheet 'Graph'
\begin{table}[htbp]
    \centering
    \ssmall
    \caption{Graph datasets.}
      \begin{tabular}{c|c|c|c|c}
      \hline
      \textbf{Graphs} & \textbf{Abbr.} & \textbf{\# nodes} & \textbf{\# edges} & \textbf{Max deg.} \bigstrut\\
      \hline\hline
      \textbf{Enron} & en    & 37K   & 183K  & 1383 \bigstrut[t]\\
      \textbf{Amazon} & az    & 334K  & 0.9M  & 549 \\
      \textbf{DBLP} & db    & 317K  & 1.0M  & 343 \\
      \textbf{Netflow} & nf    & 3.1M  & 2.9M  & 0.2M \\
      \textbf{LSBench} & lb    & 5.2M  & 20.3M & 2.3M \\
      \textbf{LiveJournal} & lj    & 4.0M  & 34.7M & 14815 \bigstrut[b]\\
      \hline
      \end{tabular}%
    \label{tab:datasets}%
\end{table}%

  
\noindent
\textbf{Platform:} All experiments are finished on a host with two Intel Xeon Gold 6226R 2.9GHz CPUs (32 cores in total) and Nvidia RTX3090 GPU. The CPUs have 512GB RAM. Each GPU has 24GB global memory. All experiments run on Ubuntu-20.04. DCSM was developed in C++ and CUDA. The CPU code was compiled using GCC 9.4.0 with O3 optimization, and the GPU code was compiled using NVCC 11.6. 

\noindent
\textbf{Datasets and query patterns:} 
Table~\ref{tab:datasets} lists the data graphs used in our experiments. All the data graphs are real-world graphs. Enron, Amazon, DBLP, and LiveJournal are networks of ground-truth communities from the SNAP dataset \cite{snapnets}. Netflow \cite{caida2019caida} is a graph of passive traffic traces. LSBench \cite{le2012linked} is a synthesized graph social network from multiple sources. All graphs are simplified to simple undirected graphs. The dynamic graphs are generated by selecting 45\% of the edges as insertion edges and 5\% as deletion edges. The nine query patterns from size-$3$ to size-$5$ used in experiments are shown in Figure~\ref{fig:queries}. The experiments include labeled and unlabeled matching. For labeled matching, we randomly assign five labels to the data graph and query pattern.

\begin{figure}[h]
    \centering
     %   \vspace{-.5em}
    % \hspace{-1em}
   \includegraphics[scale=0.42, page=15]{./fig/slides-crop.pdf}
   \caption{Query patterns for evaluation.}
   \label{fig:queries}
\end{figure}

\noindent
\textbf{Baselines:} 
We select three recent work, RapidFlow \cite{sun2022rapidflow}, CaLiG \cite{yang2023fast}, and NewSP \cite{li2024newsp} as our CPU baselines. They are three  state-of-the-art open-sourced continuous subgraph matching systems on the CPU, and have reported better performance than other CPU systems, including Graphflow \cite{kankanamge2017graphflow}, TurboFlux \cite{kim2018turboflux}, SJ-Tree \cite{choudhury2015selectivity}, IEDyn \cite{idris2017dynamic}, and Symbi \cite{min2021symmetric}. Each of them features different optimizations. 

There are two previous GPU systems: GCSM \cite{wei2024gcsm} and GAMMA \cite{qiu2024gpu}. We summarize their performance and differences in Table~\ref{tab:baseline}. Since GAMMA is not totally open-sourced, we reimplement GAMMA as GAMMAr. We evaluate them following the experimental settings in the GAMMA paper. GCSM outperforms GAMMA due to two additional optimizations absent in GAMMA: code generation, which simplifies the program, and dynamic scheduling, which reduces load imbalance among thread blocks. GCSM consistently outperforms others in all test cases, so we use GCSM as our GPU baseline. 

We also evaluate two straightforward but suboptimal strategies that can enable inter-batch parallelism: Edge-Extension and Hash-Indexing. 
Edge-Extension extends subgraphs by one edge at a time, so it does not require the neighbor arrays to be sorted, but it generates significantly more intermediate results than our algorithm, slowing down the matching step. 
Hash-Indexing stores the neighbor arrays as hash sets, but previous studies \cite{chen2022efficient} have shown that it cannot achieve optimal performance on GPU platforms.
We evaluate both approaches as baselines in the experimental section.

Another potential baseline approach is to exploit high parallelism within a small batch, such as traversing the search space in BFS order. However, previous studies \cite{wei2022stmatch, xiang2021cuts} have already extensively evaluated BFS and shown that it incurs high overhead due to intensive memory writes, large memory consumption, and thread synchronization, leading to significant performance degradation. Therefore, we did not include BFS in our baseline.

% Table generated by Excel2LaTeX from sheet 'Sheet3'
\begin{table}[htbp]
  \centering
  \ssmall
  \setlength{\tabcolsep}{2pt}
  \caption{Comparison of baselines. \textit{Avg Time} is the average execution time on four data graphs and query patterns used in GAMMA. The \textit{Avg Time} for GAMMA is taken from the original paper, while the \textit{Avg Time} of others is obtained from our own experiments.}
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
          & \multicolumn{1}{p{2.75em}|}{\textbf{Avg \newline{}Time}} & \multicolumn{1}{p{5em}|}{\textbf{Work\newline{}Stealing}} & \multicolumn{1}{p{5em}|}{\textbf{Dynamic\newline{}Scheduling}} & \multicolumn{1}{p{5em}|}{\textbf{Warp \newline{}Centric}} & \multicolumn{1}{p{5em}|}{\textbf{Redundancy \newline{}Elimination}} & \multicolumn{1}{p{5em}}{\textbf{Code\newline{}Generation}} \\
    \midrule
    \textbf{GAMMA} & 0.82  & $\checkmark$     & x     & $\checkmark$     & $\checkmark$     & x   \bigstrut[b]\\
    \textbf{GAMMAr} & 0.97  & $\checkmark$     & x     & $\checkmark$     & $\checkmark$     & x   \bigstrut[b]\\
    \textbf{GCSM} & 0.53  & $\checkmark$     & $\checkmark$     & $\checkmark$     & $\checkmark$     & $\checkmark$   \bigstrut[b]\\
    \bottomrule
    \end{tabular}%
  \label{tab:baseline}%
\end{table}%

\noindent
\textbf{Settings:}
Each GPU launches 82 thread blocks, each containing 2048 threads, which fully utilize the available active threads.
RapidFlow, CaLiG, and NewSP run on a single thread because their algorithm can only process edge updates one by one, and their implementation is based on a single-threaded design. All systems adopt the same matching order of the query pattern. Unless otherwise specified, we set the batch size each time consumed by the DCSM Graph Updater (GU) to $64$, and set its timeout threshold $x$ to 0.125ms. We allocate 4 warps to the Graph Updater, 1 warp to the Garbage Collector, and all remaining warps to the Executor. GCSM uses the same 82 blocks as DCSM, each containing 2048 threads, but all warps are used for matching.


%______________________________________________________________________________________________________________________________________________________%

\subsection{Throughput}
\begin{table}[htbp]
  \centering
  \ssmall
  \setlength{\tabcolsep}{3.5pt}
  \caption{Execution time of different systems for matching unlabeled and labeled query patterns. The '-' indicates a timeout after 8 hours. The default time unit for unlabeled matching is seconds, while for labeled matching it is milliseconds. The 's' after a number denotes seconds}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
    \hline
          & \textit{\textbf{G}} & \textbf{System}  & \textbf{Q1} & \textbf{Q2} & \textbf{Q3} & \textbf{Q4} & \textbf{Q5} & \textbf{Q6} & \textbf{Q7} & \textbf{Q8} & \textbf{Q9} \bigstrut\\
    \hline
    \hline
    \multirow{24}[8]{*}{\begin{sideways}\textbf{Unlabeled}\end{sideways}} & \multirow{6}[2]{*}{\textit{\textbf{en}}} 
                  & \textbf{Rapidflow} & 0.16 & 0.28 & 4.6 & 9.1 & 3.2 & 2.7 & 1.7 & 82.3 & 3.8 \bigstrut[t]\\
          &       & \textbf{CaliG} & 2.4 & 3.1 & 146 & 7.2 & 162 & 394 & 53 & 24332 & 1116 \\
          &       & \textbf{NewSP} & 5.2   & 1.1   & 864   & 13    & 110   & 41    & 27    & -     & 471 \\
          &       & \textbf{HashIdx}  & 0.17 & 0.2   & 3.3   & 3.8   & 1.8   & 3.4   & 0.5.  & 9.9   & 2.7  \\
          &       & \textbf{Edge-Ext}   & 0.1  & 0.1   & 1.5   & 2.1   & 1.6   & 8.4   & 8.8   & 92    & 12 \\
          &       & \textbf{DCSM}       & 0.1  & 0.1   & 1.5   & 2.1   & 0.7   & 1.2   & 0.2   & 7.1   & 1.2 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{az}}} 
                  & \textbf{Rapidflow} & 1.1   & 2.4   & 3.7   & 1.9   & 4.6   & 5.2   & 4.2   & 7.8   & 5.1 \bigstrut[t]\\
          &       & \textbf{CaliG} & 4.1   & 5.7   & 15    & 7.8   & 32    & 39    & 33    & 230   & 379 \\
          &       & \textbf{NewSP} & 2.2   & 1.9   & 28    & 5.2   & 13    & 5.1   & 4.6   & 92    & 8.4 \\
          &       & \textbf{HashIdx}  & 0.25 & 0.27 & 0.36 & 0.23 & 0.31 & 0.58 & 0.22 & 0.39 & 0.71 \\
          &       & \textbf{Edge-Ext} & 0.09  & 0.1   & 0.1   & 0.1   & 0.1   & 0.5   & 1.2   & 5.5   & 3.2 \\
          &       & \textbf{DCSM} & 0.09  & 0.1   & 0.2   & 0.1   & 0.2   & 0.2   & 0.1   & 0.3   & 0.4 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{db}}} 
                  & \textbf{Rapidflow} & 1.1   & 2.6   & 5.2   & 3.2   & 5.5   & 6.7   & 6.6   & 29    & 16 \bigstrut[t]\\
          &       & \textbf{CaliG} & 5.4   & 7.8   & 54    & 12    & 102   & 152   & 101   & 5202  & 7638 \\
          &       & \textbf{NewSP} & 4.8   & 3.1   & 181   & 9.1   & 64    & 45    & 76    & 3861  & 7363 \\
          &       & \textbf{HashIdx}  & 0.16 & 0.27 & 1.6 & 0.26 & 0.96 & 1.4 & 1.4 & 4.8 & 1.6  \\
          &       & \textbf{Edge-Ext} & 0.05  & 0.1   & 1.2   & 0.2   & 1.2   & 2.8   & 3.4   & 116   & 11 \\
          &       & \textbf{DCSM} & 0.05  & 0.1   & 1.2   & 0.2   & 0.5   & 0.6   & 0.6   & 1.8   & 0.6 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{nf}}} 
                  & \textbf{Rapidflow} & 10.1  & 4.4   & 7.2   & 1278  & 5.8   & 5.5   & 5.1   & 6.2   & 5.9 \bigstrut[t]\\
          &       & \textbf{CaliG} & 247   & 58    & 238   & 1508   & 88    & 1678  & 1525  & 244   & 206 \\
          &       & \textbf{NewSP} & 1654  & 2.7   & 1384  & 4027  & 480   & 5.8   & 1.8   & 803   & 2.1 \\
          &       & \textbf{HashIdx}  & 2.7 & 0.93 & 2.5 & 464 & 1.7 & 2.3 & 1.4 & 2.5 & 3.5  \\
          &       & \textbf{Edge-Ext} & 1.2   & 0.6   & 1.2   & 232   & 1.2   & 1.1   & 1.7   & 21    & 4.8 \\
          &       & \textbf{DCSM}     & 1.2   & 0.6   & 1.2   & 232   & 0.9   & 0.9   & 0.8   & 1.7   & 1.2 \bigstrut[b]\\
    \hline
    \hline
    \multirow{36}[12]{*}{\begin{sideways}\textbf{Labeled}\end{sideways}} & \multirow{6}[2]{*}{\textit{\textbf{en}}} 
                  & \textbf{Rapidflow} & 27    & 78    & 37    & 38    & 133   & 102   & 127    & 996   & 42 \bigstrut[t]\\
          &       & \textbf{CaliG} & 32    & 44    & 188   & 72    & 299   & 408   & 164   & 7190  & 555 \\
          &       & \textbf{NewSP} & 95    & 61    & 1283  & 135   & 306   & 125   & 105   & 6633  & 210 \\
          &       & \textbf{HashIdx} & 20 & 9.4 & 19 & 23 & 28 & 49 & 55 & 221 & 80  \\
          &       & \textbf{Edge-Ext} & 7     & 6     & 10    & 11    & 10    & 50    & 62    & 98    & 168 \\
          &       & \textbf{DCSM} & 7     & 6     & 10    & 11    & 10    & 21    & 21    & 91    & 27 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{az}}} 
                  & \textbf{Rapidflow} & 71    & 110   & 171   & 98    & 174   & 166   & 104   & 204   & 107 \bigstrut[t]\\
          &       & \textbf{CaliG} & 91    & 174   & 141   & 114   & 241   & 334   & 91    & 378   & 387 \\
          &       & \textbf{NewSP} & 283   & 263   & 359   & 309   & 324   & 301   & 274   & 433   & 267 \\
          &       & \textbf{HashIdx} &  91 & 43 & 48 & 71 & 85 & 53 & 88 & 50 & 66 \\
          &       & \textbf{Edge-Ext} & 31    & 32    & 31    & 33    & 33    & 49    & 52    & 178   & 395 \\
          &       & \textbf{DCSM} & 31    & 32    & 31    & 33    & 33    & 32    & 32    & 32    & 45 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{db}}} 
                  & \textbf{Rapidflow} & 97    & 123   & 172   & 111   & 184   & 238   & 122   & 372   & 179 \bigstrut[t]\\
          &       & \textbf{CaliG} & 114   & 177   & 252   & 172   & 433   & 647   & 997   & 3945  & 13s \\
          &       & \textbf{NewSP} & 299   & 282   & 704   & 352   & 498   & 433   & 494   & 3885  & 4.8s \\
          &       & \textbf{HashIdx} & 53 & 56 & 82 & 35 & 60 & 106 & 53 & 85 & 69  \\
          &       & \textbf{Edge-Ext} & 31    & 30    & 31    & 27    & 30    & 82    & 48    & 106   &  917 \\
          &       & \textbf{DCSM} & 31    & 30    & 31    & 27    & 30    & 42    & 27    & 32    &  35 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{nf}}} 
                  & \textbf{Rapidflow} & 1.1s   & 495   & 0.7s   & 154s  & 608   & 719   & 609   & 715   & 1007 \bigstrut[t]\\
          &       & \textbf{CaliG}     & 1.7s   & 639   & 1.2s   & 611s  & 758   & 1247  & 1544  & 969   & 3053 \\
          &       & \textbf{NewSP}     & 18s   & 1183   & 11s   & 48s   & 4387  & 1164  & 1190  & 5083  & 1195 \\
          &       & \textbf{HashIdx} & 0.67 & 749 & 1.1 & 40 & 427 & 215 & 285 & 260 & 185  \\
          &       & \textbf{Edge-Ext}      & 0.4s  & 116   & 0.4s  & 18s   & 149   & 373   & 501   & 837   &  1834 \\
          &       & \textbf{DCSM}      & 0.4s  & 116   & 0.4s  & 18s   & 149   & 110   & 101   & 96    &  128 \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{lb}}} 
                  & \textbf{Rapidflow} & 2.3s  & 2.5s  & 12s   & 455s  & 3.1s  & 2.9s  & 2.5s  & 173s  & 3.1s \bigstrut[t]\\
          &       & \textbf{CaliG} & 9.5s  & 11s   & 449s  & 575s  & 80s   & 713s  & 60s   & 16973s & 961s \\
          &       & \textbf{NewSP} & 22.8s & 5.1s  & 2625s & 497s  & 77s   & 6.3s  & 5.8s  & 9953s & 6.1s \\
          &       & \textbf{HashIdx} & 2.9s & 3.2s & 1.4s & 15s & 2.3s & 1.8s & 0.43s & 26s & 1.5s  \\
          &       & \textbf{Edge-Ext} & 1.3s  & 1.3s  & 1.1s  & 6.2s  & 1.3s  & 2.2s  & 1.1s  & 80s   &  4.2s \\
          &       & \textbf{DCSM} & 1.3s  & 1.3s  & 1.1s  & 6.2s  & 1.3s  & 1.1s  & 0.2s  & 16s   & 0.9s \bigstrut[b]\\
\cline{2-12}          & \multirow{6}[2]{*}{\textit{\textbf{lj}}} 
                  & \textbf{Rapidflow} & 4.8s  & 7.1s  & 17s   & 17s   & 12s   & 92s   & 9.7s  & 101s  & 25s \bigstrut[t]\\
          &       & \textbf{CaliG} & 30s   & 45s   & 265s  & 62s   & 264s  & 1124s & 223s  & 22594s & 5550s \\
          &       & \textbf{NewSP} & 26s   & 17s   & 1284s & 39s   & 333s  & 111s  & 125s  & -     & 5129s \\
          &       & \textbf{HashIdx} & 1.9s & 1.8s & 2.8s & 3.7s & 2.6s & 25s & 1.5s & 29s & 8.9s \\
          &       & \textbf{Edge-Ext} & 1.8s  & 2.9s  & 3.3s  & 3.5s  & 2.2s  & 76s   & 1.9s  & 282s  & 76s  \\
          &       & \textbf{DCSM}     & 0.8s  & 1.0s  & 1.3s  & 1.5s  & 1.2s  & 10s   & 1.4s  & 13s   & 3.2s   \bigstrut[b]\\
    \hline
    \end{tabular}%
  \label{tab:throughput}%
\end{table}%

In this section, we compare the throughput of DCSM with that of other systems. We set the update rate to the maximum (i.e., all updates arrive at time $0$) to measure how long each system takes to process all updates. A shorter processing time indicates that the system has higher throughput.
The graph updater (GU) of DCSM consumes the updates with a batch size of $64$; this setting can ensure the best performance.

\noindent
\subsubsection{Comparison with the CPU Systems}
Table~\ref{tab:throughput} shows the processing time of RapidFlow, CaliG, NewSP, and DCSM for matching query patterns Q1-Q9 on different data graphs. DCSM runs on a single GPU. In most test cases, RapidFlow outperforms the other two CPU-based systems, achieving the best performance on CPU. DCSM further outperforms RapidFlow, achieving speedups ranging from $1.7$x to $42$x, with an average of $10.5$x. The experiment result indicates the effectiveness of our system. The speedup mainly comes from the massive parallelism of the GPU and the various optimizations we proposed. 

For unlabeled matching, on the four graphs—Enron, Amazon, DBLP, and Netflow—DCSM achieves average speedups of $6.2$x, $22$x, $16$x, and $8.6$x over RapidFlow. For labeled matching on the four small graphs—Enron, Amazon, DBLP, and Netflow—DCSM achieves average speedups of $6.7$x, $4.2$x, $5.6$x, and $5.1$x over RapidFlow; on the two large graphs, LSBench and LiveJournal, the average speedup increases significantly to $14.5$x and $13$x. This is because the workload on small graphs is already low, even without labels, and adding labels to both the graphs and query patterns further reduces it. As a result, the search space may be limited to only a few thousand elements—or even just a few hundred. In such cases, warps experience higher overhead when competing for updates, as they may spend more time waiting to acquire the queue lock than performing actual matching.

In some cases, such as az-Q7, nf-Q3, and en-Q2, NewSP and CaliG can slightly outperform RapidFlow; this is because they include optimizations for specific cases, such as the reordering of extensions and operations in NewSP. In most cases, RapidFlow achieves better performance, since the local index used in RapidFlow greatly reduces the search space. Our system does not incorporate RapidFlow's local index method, resulting in a larger search space. Therefore, DCSM does not achieve the order-of-magnitude speedup over RapidFlow comparable to GPU's computational advantage over CPU. Nevertheless, we still outperform RapidFlow through massive parallelism.

\subsubsection{Comparison with the GPU Naive Methods}

As introduced in Section~\ref{sec:exp_setup}, we have two intuitive methods to enable inter-batch parallelism, but both have limitations. In this section, we compare the throughput of DCSM with these two naive methods: Hash-Indexing and Edge-Extension.

From Table~\ref{tab:throughput}, we can see that DCSM consistently outperforms Hash-Indexing. The Hash-Indexing method does not work well for GPU-based subgraph matching tasks, as it causes more thread divergence within a warp during set operations.

Edge-Extension performs especially well on sparse query patterns but is slower on dense query patterns (Q7, Q8, and Q9). This is because the time complexity of Edge-Extension is $O(n^{|E(Q)|})$, which is higher than DCSM’s $O(n^{|V(Q)|})$ on dense query patterns. As a result, Edge-Extension explores a larger search space than DCSM on dense query patterns. However, Edge-Extension still outperforms most CPU-based systems, as the high parallelism of GPUs offsets the drawbacks introduced by its algorithmic complexity.

\subsubsection{Comparison with GCSM}


% Table generated by Excel2LaTeX from sheet 'Sheet4'
\begin{table}[htbp]
  \centering
  \ssmall
  \setlength{\tabcolsep}{4.8pt}
  \caption{Performance comparison between GCSM and DCSM for matching unlabeled and labeled query patterns. The table shows the batch size $b$ used by GCSM. When the batch size is set to $b$, GCSM and DCSM exhibit nearly the same processing time. Both GCSM and DCSM are executed on a single GPU.}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}
    \hline
          & \textit{\textbf{G}} & \textbf{Q1} & \textbf{Q2} & \textbf{Q3} & \textbf{Q4} & \textbf{Q5} & \textbf{Q6} & \textbf{Q7} & \textbf{Q8} & \textbf{Q9} \bigstrut\\
    \hline
    \hline
    \multirow{4}[8]{*}{\begin{sideways}Unlabeled\end{sideways}} 
                          & \textit{\textbf{en}} & 7456 & 5984 & 7264 & 5216 & 6784 & 5600 & 6304 & 7136 & 5088  \bigstrut\\
    \cline{2-11}          & \textit{\textbf{az}} & 5600 & 6176 & 7136 & 6144 & 6880 & 7232 & 6848 & 6976 & 5632  \bigstrut\\
    \cline{2-11}          & \textit{\textbf{db}} & 6400 & 5824 & 7520 & 6208 & 7200 & 5120 & 7648 & 6496 & 5152  \bigstrut\\
    \cline{2-11}          & \textit{\textbf{nf}} & 6528 & 6176 & 6144 & 4864 & 6400 & 5824 & 4672 & 6272 & 5120  \bigstrut\\
    \hline
    \hline
    \multirow{6}[12]{*}{\begin{sideways}Labeled\end{sideways}} 
                          & \textit{\textbf{en}} & 6976 & 10848 & 6464 & 7712 & 10720 & 7584 & 8384 & 11008 & 8640 \bigstrut\\
    \cline{2-11}          & \textit{\textbf{az}} & 8736 & 7328 & 6816 & 7968 & 8256 & 9888 & 6592 & 9504 & 7232 \bigstrut\\
    \cline{2-11}          & \textit{\textbf{db}} & 9184 & 6752 & 7968 & 9024 & 6688 & 10304 & 9824 & 7424 & 8096 \bigstrut\\
    \cline{2-11}          & \textit{\textbf{nf}} & 8448 & 8832 & 10976 & 9792 & 9632 & 8160 & 8288 & 10080 & 9312 \bigstrut\\
    \cline{2-11}          & \textit{\textbf{lb}} & 6880 & 6144 & 6560 & 5536 & 5088 & 6112 & 5376 & 5056 & 5824 \bigstrut\\
    \cline{2-11}          & \textit{\textbf{lj}} & 4768 & 7520 & 6592 & 5504 & 7072 & 5504 & 6560 & 6816 & 5696 \bigstrut\\
    \hline
    \end{tabular}%
  \label{tab:gcsm_ul}%
\end{table}%


Table~\ref{tab:gcsm_ul} shows the performance comparison between GCSM and DCSM for unlabeled and labeled continuous subgraph matching. As introduced in Section~\ref{sec:Intro}, GCSM processes updates in fixed-size batches, which leads to low GPU utilization when the batch size is small. The graph updater (GU) of DCSM also consumes updates in fixed-size batches. We identify a batch size $b$ at which GCSM and DCSM achieve the same processing time. While GCSM's processing time decreases as $b$ increases, DCSM's execution time remains stable regardless of the batch size, because DCSM can process multiple batches in parallel.

We observe that the numbers in Table~\ref{tab:gcsm_ul} roughly correspond to the number of active warps on the GPU. GCSM achieves the same performance as DCSM when processing with a batch size large enough to saturate GPU resources. For some small labeled data graphs, such as en, az, and db, GCSM requires a larger batch size, because a smaller batch is processed too quickly to effectively hide the kernel launch time for the next batch.


Figure~\ref{fig:scale} shows the trend of GCSM and DCSM performance with the batch size. GCSM's processing time becomes longer on small batch sizes, while DCSM remains stable regardless of the batch size. If all edge updates in the test data are grouped into an extremely large batch, GCSM has almost the same performance as DCSM. We also tested GPU utilization under different batch sizes. We can see that as the batch size decreases, DCSM's GPU utilization remains constant while GCSM's GPU utilization decreases roughly by half successively.


\begin{figure}
    \centering
    \subfloat[]{
        \includegraphics[scale=0.38, page=1]{./fig/gcsm-crop.pdf}
        \label{fig:gcsm}
    }%
    \subfloat[]{
        \includegraphics[scale=0.38, page=1]{./fig/gcsm2-crop.pdf}
        \label{fig:gcsm2}
    }
    \caption{(a) and (b) show the trend of GCSM performance with batch size $b$ on unlabeled db-Q5 and labeled lj-Q5. "All" refers to the batch size being equal to the total number of edge updates in the test data. The numbers marked on the line stand for GPU utilization.}
    \label{fig:scale}
\end{figure}


%______________________________________________________________________________________________________________________________________________________%
\subsection{Response Time}
\begin{figure}[h]
    \centering
    \subfloat[Netflow Q4]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/nfq4-crop.pdf}
        \label{fig:rt1}
    }%
    \subfloat[Netflow Q6]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/nfq6-crop.pdf}
        \label{fig:rt2}
    }\hfil
    \subfloat[LSBench Q4]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/lbq4-crop.pdf}
        \label{fig:rt3}
    }%
    \subfloat[LiveJournal Q9]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/ljq9-crop.pdf}
        \label{fig:rt4}
    }\hfil
    \subfloat[LiveJournal Q8]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/ljq8-crop.pdf}
        \label{fig:rt5}
    }%
    \subfloat[LSBench Q8]{
        \centering
        \includegraphics[scale=0.4, page=1]{./fig/lbq8-crop.pdf}
        \label{fig:rt6}
    }\hfil
    \caption{Response time over different update rates. The time unit is seconds for Netflow Q4 and milliseconds for all others. GCSM-4096 refers to GCSM configured with a batch size of 4096.}
    \label{fig:response_time}
\end{figure}

In this section, we compared the response times of DCSM, GCSM, and RapidFlow over different update rates (updates / second). 
Figure~\ref{fig:response_time} shows the experimental results.

By observing real-world data, such as Twitter traffic and real-time updates in social networks, we found that in reality, the time intervals between two consecutive arriving updates in dynamic graphs follow an exponential distribution. 
We generated timestamps for the updates in the edge stream for each data graph according to this exponential distribution.
By setting the parameters of the exponential distribution, we can control the time span between the last update and the first update.
A longer time span means fewer updates arrive per unit time, which has a lower $\Delta e_k$ update rate. Conversely, a shorter time span has a higher $\Delta e_k$ update rate.
The horizontal axis in Figure~\ref{fig:response_time} represents update rate. 

The experimental results confirm the effectiveness of the DCSM. 
As shown in Figure~\ref{fig:response_time}, RapidFlow exhibits higher response time at high update rates because the processing capability of a single CPU is limited and cannot guarantee RapidFlow's high throughput. 
At low update rates, GCSM takes longer to form a batch, which also leads to increased response time. 
DCSM can effectively handle various update rates. The parallelism between batches enables DCSM to achieve optimal performance regardless of the update rate. Therefore, DCSM can flexibly adapt to traffic fluctuations over time in real-world scenarios.

We also compared against GCSM-128. However, due to GCSM-128's extremely low throughput, it cannot handle the update rates within the range shown in Figure~\ref{fig:response_time}; the GCSM-128 curve appears as a nearly flat line above RapidMatch. In Figures~\ref{fig:rt1} to~\ref{fig:rt6}, DCSM achieves 70.7x, 53.9x, 60.4x, 80.3x, 87.9x, and 60.4x higher throughput compared to GCSM-128, respectively. GCSM-128's response time drops to near-zero at an extremely low update rate, but GCSM with small batch size performs far worse than CPU systems under typical update rates.


\begin{figure}
    \centering
    \subfloat[Netflow Q4]{
        \includegraphics[scale=0.45, page=1]{./fig/gpuutil-crop.pdf}
        \label{fig:util1}
    }%
    \subfloat[Netflow Q6]{
        \includegraphics[scale=0.45, page=1]{./fig/gpuutil2-crop.pdf}
        \label{fig:util1}
    }
    \caption{GPU utilization vs. update rate (k: thousand, m: million) for different systems}
    \label{fig:gpuutil}
\end{figure}

Figure~\ref{fig:gpuutil} illustrates GPU utilization as a function of the update rate. 
DCSM consistently achieves higher GPU utilization than GCSM across all update rates. 
With large batch sizes, GCSM exhibits low utilization due to its batching strategy, which delays the response time, whereas with small batch sizes, insufficient parallelism limits utilization. 
By contrast, DCSM dynamically schedules incoming updates to available warps, enabling high GPU utilization across update rates. 
At high update rates, the GPU utilization of DCSM, GCSM-500, and GCSM-10000 converges to 100\%.


%______________________________________________________________________________________________________________________________________________________%
\subsection{Overhead Analysis}
\label{sec:overhead}
% Table generated by Excel2LaTeX from sheet 'Sheet7'
\begin{table}[htbp]
  \centering
  \ssmall
  \caption{Time (ms) spent by the graph updater (GU) to synchronously consume all updates.}
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \hline
          & Batch Size & en    & az    & db    & nf    & lb    & lj \bigstrut\\
    \hline
    \hline
    \multirow{5}[2]{*}{\begin{sideways}Unlabeled\end{sideways}} 
          & 1     & 59    & 68    & 114   & 1050  & x     & x \bigstrut[t]\\
          & 32    & 7     & 27    & 11    & 122   & x     & x \\
          & 128   & 5     & 16    & 9     & 70    & x     & x \\
          & 512   & 3     & 3     & 9     & 67    & x     & x \\
          & 2048  & 3     & 3     & 9     & 57    & x     & x \bigstrut[b]\\
    \hline
    \hline
    \multirow{5}[2]{*}{\begin{sideways}Labeled\end{sideways}} 
          & 1     & 11    & 7     & 12    & 210   & 513   & 1155 \bigstrut[t]\\
          & 32    & 7     & 2     & 2     & 24    & 52    & 104 \\
          & 128   & 5     & 2     & 2     & 14    & 41    & 81 \\
          & 512   & 3     & 2     & 2     & 13    & 39    & 76 \\
          & 2048  & 3     & 2     & 2     & 11    & 39    & 77 \bigstrut[b]\\
    \hline
    \end{tabular}%
  \label{tab:gu_time}%
\end{table}%


The MVG data structure causes DCSM to perform more data copying than other systems, which increases memory access overhead.

Table~\ref{tab:gu_time} reports the time required for the graph updater (GU) to process all updates and apply them to the MVG. 
These data were collected while GU, EX, and GC were running concurrently, rather than with GU running in isolation.

By comparing Table~\ref{tab:gu_time} and Table~\ref{tab:throughput}, it can be observed that, in most cases, the time required by GU is significantly lower than that for incremental matching. 
This indicates that GU can produce updates faster than the executor (EX) can consume them, even when updating the MVG with a batch size of 1. 
Thus, EX is the performance bottleneck in most cases.

GU is slower than EX only when DCSM matches size-3 query patterns and GU uses a batch size of 1. 
For example, in Figure~\ref{tab:throughput}, lj-Q1 requires 800 ms for matching, while GU takes 1155 ms; nf-Q5 requires 800 ms, while GU takes 1050 ms.
However, in DCSM, the batch size of GU is typically set to 64 or larger. Therefore, at high update rates, GU does not become a performance bottleneck. At low update rates, timeouts may cause GU to update the MVG with a batch size of 1, but this millisecond-level slowdown does not noticeably increase response time or affect user experience.

We also profiled DCSM's memory footprint. At high update rates, GPU memory consumption rises sharply within a few milliseconds and then decreases gradually at a rate comparable to that of EX, as GC operates at a speed determined by EX, which is much faster than GU. At update rates equal to or lower than the processing speed of EX, GPU memory consumption remains stable over time.


% 在一些轻量级的query任务上，比如lj-Q1， nf-Q5
%______________________________________________________________________________________________________________________________________________________%
\subsection{Ablation Study}
\begin{figure}
    \centering
    \subfloat[Batch Size = $64$]{
        \includegraphics[scale=0.45, page=1]{./fig/speedups-crop.pdf}
        \label{fig:bs1}
    } \\
    \subfloat[Batch Size = $128$]{
        \includegraphics[scale=0.45, page=1]{./fig/speedups2-crop.pdf}
        \label{fig:bs2}
    } 
    \caption{Speedups brought by parallel graph updater on different data graphs. Naive refers to a single warp sequentially copying different arrays, rather than copying multiple arrays in parallel.}
    \label{fig:async_gu_data}
\end{figure}


Figure~\ref{fig:async_gu_data} shows the speedup achieved by the parallel graph updater (GU) across different data graphs. 

We observe that assigning more warps to the GU achieves better speedups, as more warps can issue more in-flight memory request instructions in parallel within the same cycle, thereby saturating the GPU global memory bandwidth. 
Higher parallelism ensures that memory throughput is not bounded by instruction dispatch.

However, when the number of warps continues to increase such that the total number of threads exceeds the batch size, further increasing warps no longer brings significant speedup. 
For example, in Figure~\ref{fig:bs1}, using 8 warps produces almost the same speedup as using 4 warps.
This is because different batches cannot be updated to the MVG in parallel, and the average neighbor size in the data graph is very small, resulting in the total length of all arrays to be copied being smaller than the number of available threads.

We also observe that a single warp achieves the most significant speedup compared to Naive, where one warp iterates 32 times.
Since most neighbor arrays have a size of only 1-2, a single warp in parallel only needs to iterate 2 to 3 times. Single warp parallelization greatly reduces the number of iterations and increases memory request parallelism.
Meanwhile, 2 warps only halve the number of iterations compared to 1 warp.

Our proposed pipeline processing method also brings significant speedup to the GU, and becomes particularly effective under high update rates. The copy operation of the subsequent batch can hide the sort operation of the pr

Through these optimizations, the GU can process updates at high speed, thereby avoiding performance bottlenecks that would be slower than EX for most queries.
%______________________________________________________________________________________________________________________________________________________%

\section{Related Works}
Continuous subgraph matching (CSM) originates from static subgraph matching. The design of static subgraph matching systems has a significant impact on CSM.
There are many efforts to design high performance systems for subgraph matching. 
These include CPU systems, such as GraphZero \cite{mawhirter2019graphzero}, Dryadic \cite{mawhirter2021dryadic}, and GraphPi\cite{shi2020graphpi}, Peregrine \cite{jamshidi2020peregrine}, Automine \cite{mawhirter2019automine}, G2Miner \cite{chen2022efficient},  Sandslash \cite{chen2021sandslash}, DecoMine \cite{chen2022decomine}, and RapidMatch \cite{sun2020rapidmatch}
These also include  GPU systems, such as Gunrock \cite{wang2020fast}, GPSM \cite{tran2015fast}, GSI \cite{zeng2020gsi}, cuTS \cite{xiang2021cuts}, PBE \cite{guo2020gpu}, and STMatch \cite{wei2022stmatch}.  These systems introduce various program optimization methods to improve hardware utilization. 

Continuous subgraph matching has been studied for decades on CPU. IncIsoMatch \cite{fan2013incremental} is the first continuous subgraph matching work, it retrieves the graph region affected by the edge update of the query and performs the static subgraph matching again in the region to get the incremental results.
Graphflow \cite{kankanamge2017graphflow} eliminates the repeated matching in IncIsoMatch for each edge update and derives the multi-way join equations for batched continuous graph matching, performing incremental matching based on the equations. SJ-Tree \cite{choudhury2015selectivity} builds an index tree for computing incremental matching results. It stores partial results to eliminate redundant computation that can be precomputed before runtime, but the index tree leads to memory explosion when processing large graphs.  TurboFlux \cite{kim2018turboflux} proposes a new data structure called data-centric graph structure that can reduce the storage amount of partial results to get a better tradeoff between memory consumption and computation overhead. SymBi \cite{min2021symmetric} is the successive work of TurboFlux and proposes a pruning method for candidate vertices with query edges. Rapidflow \cite{sun2022rapidflow} builds a local index for breaking the matching order fixation and a dual matching elimination to reduce the redundant computation caused by query automorphisms. CaliG \cite{yang2023fast} divides the query into two parts, it computes the partial results with the first part and gets the final result with the second part, the extension in the second part can be finished without backtracking. NewSP \cite{li2024newsp} gets the best performance on the CPU. NewSP avoids premature expansions of the search space by postponing expansion at the operation level.

In recent years, some studies \cite{wei2024gcsm, qiu2024gpu} have started using GPUs to accelerate continuous subgraph matching. GCSM \cite{wei2024gcsm} designs a sampling algorithm for memory optimization, it only places frequently accessed vertices of graphs on the GPU to support extremely large graphs that exceed the GPU memory size. GAMMA \cite{qiu2024gpu} is batch-dynamic subgraph matching with warp-level work stealing and coalesced search for reducing redundant computations. Both GCSM and GAMMA are offline systems, based on the assumption that we can form an extremely large batch size.


\section{Conclusion}
This paper presents the first continuous subgraph matching system on GPU capable of different batches in parallel. It offers a highly practical solution for various real-world scenarios. However, DCSM also introduces new challenges, such as memory inefficiency due to volatile memory and deeply hidden redundant computations that could be eliminated during preprocessing. These issues are not present in previous systems. Many previous works cache a large number of intermediate results to avoid recomputing incremental matching from scratch. A new challenge arises in how to manage these intermediate results efficiently on GPUs and how to make the indexing structures used to store them compatible with our system. There is still significant room for optimizing DCSM in terms of both performance and efficiency. 