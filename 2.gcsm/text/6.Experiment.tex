\section{Experimental Results}
\label{sec:Experiment}
This section provides an evaluation of \texttt{GCSM} by comparing it with three naive GPU implementations and two CPU systems. 


\subsection{Experimental Setup}
\iffalse
\textcolor{red}{ In this section, we first compare our system with six continuous subgraph matching(CSM) systems: Graphflow~\cite{kankanamge2017graphflow}, SJ-Tree~\cite{choudhury2015selectivity}, IEDyn~\cite{idris2017dynamic}, TurboFlux~\cite{kim2018turboflux}, SymBi~\cite{min2021symmetric} and Rapidflow~\cite{sun2022rapidflow}. As most of the CSM systems except for SJ-Tree and Rapidflow are not open-sourced, we rely on the code provided by InDepth~\cite{sun2022depth} which re-implements the four not open-sourced CSM systems along with the source code of SJ-Tree and Rapidflow. Then we give the breakdown times of our system, we provide some statistical data about the proportions of sampling, data transfer, and graph update time to the total end-to-end matching time, as well as the amount of data transmission over PCI-E. We further compare our system with two intrinsic NVIDIA CPU-GPU communication methods: Unified Memory(UM) and Zero Copy, and the k-hops neighbor caching method provided by VSGM. In the last part of this section, we test the accuracy of the sampling method we proposed by giving the overlap between the real most frequently accessed vertices and the sampled vertices. }
\fi

\noindent
\textbf{Platform:} Our experiments are conducted on a CPU-GPU system with two Intel Xeon Gold 6226R 2.9GHz CPUs (32 cores in total) and an Nvidia RTX3090 GPU. The GPU is connected to the CPUs through PCIe. The CPUs have 512GB RAM, and the GPU has 24GB global memory. 
The test platform runs a Ubuntu-20.04. Our system was developed in C++ and CUDA (which is the language provided by Nvidia to program its GPUs).
All the CPU code was compiled using GCC 9.4.0 with O3 optimization, and the GPU code was compiled using NVCC 11.2. 

%\textcolor{red}{GPU: Nvidia RTX3090 GPUs. The memory capacity of each RTX3090 is 24G. The CUDA code was compiled with NVCC 11.2. The CUDA kernel for incremental matching is launched with 82 blocks, each of which has 1024 threads.}

\input{table/graph.tex}
\input{fig/6.QueryVis.tex}

\noindent
\textbf{Datasets and query graphs:} 
Table~\ref{tab:datasets} lists the data graphs used in our experiments. 
Among the data graphs, AZ, LJ, PA, CA, and FR are from the SNAP dataset~\cite{snapnets}, while SF3K and SF10K are random graphs generated by the LDBC graphalytics~\cite{iosup2016ldbc}. 
Following the existing research on streaming graphs~\cite{sun2022rapidflow, sun2022depth, kim2018turboflux}, we generate dynamic graphs from static graphs. 
For FR, SF3K, and SF10K, 
we randomly select $12\times 8192$ edges from each data graph to construct the edge updates. 
For AZ, LJ, PA, and CA, we randomly select 10\% of edges from the original graph, following the settings in~\cite{sun2022rapidflow, sun2022depth}.
Each selected edge is marked as either insertion or deletion with equal probability. The edges marked for insertion are removed from the data graph. If all the incident edges of a vertex are removed, the vertex is also removed from the graph. 
We use six query graphs from size-5 to size-7, as shown in Fig.~\ref{fig:queries}. 

\noindent
\textbf{Baselines:} 
We compare our system with four naive GPU implementations. The first implementation (\texttt{UM}) uses GPU unified memory. All the neighbor lists of the data graph are allocated as unified memory on the CPU and are directly accessed by the GPU kernel in pages during the matching procedure. 
The second GPU implementation (\texttt{ZP}) uses GPU zero-copy access. All the neighbor lists are allocated as pinned memory on the CPU and are mapped to the GPU address space. 
The GPU kernel can directly access the neighbor lists on the CPU in cache lines. 
The third GPU implementation (\texttt{VSGM}) employs the caching technique proposed in~\cite{10.5555/3571885.3571954}, which copies the neighbor lists of all $k$-hop neighbors of the updated edges onto the GPU before matching. During the matching, the GPU kernel only accesses GPU memory. 
The fourth GPU baseline (\texttt{Naive}) adopts a similar configuration to our system. It caches the neighbor lists of certain nodes on the GPU while keeping most of the graph data in CPU memory. However, it uses node degree as an estimate of access frequency.
For a fair comparison, 
all the GPU versions use the same GPU kernel adapted from STMatch~\cite{wei2022stmatch} for the matching task. 

We also compare our system with two CPU baselines. 
The first baseline is RapidFlow (\texttt{RF})~\cite{sun2022rapidflow}, which is a state-of-the-art continuous subgraph matching system on the CPU featuring optimized matching order. 
To achieve the matching order optimization, RapidFlow uses an index data structure to store the candidate vertices for each pattern vertex. 
The index data structure consumes a lot of memory and causes system crashes for large graphs. Therefore, we compare our system with RapidFlow only using small graphs (AZ and LJ) for which RapidFlow does not crash on our platform. 
To show the benefit of GPU processing for large graphs, we implement a CPU system based on the nested loops in Fig.~\ref{fig:for_loop}, which always start the matching process from the updated edges. 
For a fair comparison, our CPU code uses the same stack-based implementation~\cite{wei2022stmatch} and the same matching order as our GPU code. 

\noindent
\textbf{Settings:}
We execute the GPU code by launching 82 thread blocks, each containing 1024 threads on the GPU. The CPU code is run with 
32 CPU threads. The original code of RapidFlow was single-threaded, so we parallelized its outermost loop that iterates over the candidate vertices for the first pattern vertex. 
Our CPU code is also parallelized at the outermost loop that iterates over the updated edges. 
%We test the performance with two different batch size configurations: $|\Delta E|=$ 4096 and 8192. 
For our GPU system, we set 
the number of random walks $M$ to $|\Delta E|D^{n-2}/32^n$. 
Since the matching kernel (STMatch) uses about 10GB GPU memory, the maximum GPU buffer size is set to 14GB.
Nodes with the highest estimated frequency are cached in the GPU buffer. In all our testcases, the neighbor lists of all nodes sampled by the random walk procedure take less than 2GB and fit in the GPU buffer. Since a node is sampled at least once, this means that nodes with an estimated frequency greater or equal to $|\Delta E|$ are cached on GPU.



\iffalse
\textcolor{red}{Amazon, LiveJournal, Netflow and LSBench are the small graphs used in ~\cite{sun2022depth}. We conduct experiments on these graphs for comparing our system with previous CSM systems. Com-friendster, twitter\_mpi, datagen-sf3k-fb, graph500-28, datagen-sf10k-fb are the big graphs that exceed the GPU's memory capacity. We use these graphs for doing caching performance comparisons between our system and unified memory, zero-copy and VSGM. Orkut is a small graph used for testing the performance of the overhead of our system.} \fi











% \input{table/speedup.tex}


\subsection{Comparison with Naive GPU Implementations}
\label{sec:overall_performance}

\input{fig/6.Performance.tex}


Fig.~\ref{fig:performance_fr} to Fig.~\ref{fig:performance_roadnet} show the average execution time of different implementations for one batch of edge updates. 
For different data graphs and query patterns, 
our system is consistently faster than the naive zero-copy implementation (\texttt{ZP}), achieving 1.4 to 2.9x speedups, with an average of 1.81x. 
The speedups are due to the reduced data access to the neighbor lists on the CPU. As labeled in the figures, our caching mechanism reduces the CPU memory access by 1.3x to 6.7x times compared with naive zero-copy. 

%We can see that during the matching procedure, both \texttt{GCSM32} and \texttt{GCSM64} access less data from CPU than the naive zero-copy implementation. 
%\texttt{GCSM32} is in general faster than \texttt{GCSM64} due to the even smaller data access sizes. 
%This is because \texttt{GCSM32} obtains a more accurate estimation of the frequent vertices with more random walk samples.



\begin{table}[t]
  \centering
  \footnotesize
  \caption{Overhead of frequency estimation (FE) and data copying (DC) in percentage of total execution time.}
    \label{tab:breakdown}%
    \begin{tabular}{|r|r|r||r|r||r|r|}
    \hline
    \multirow{2}[4]{*}{} & \multicolumn{2}{c||}{FR} & \multicolumn{2}{c||}{SF3K} & \multicolumn{2}{c|}{SF10K} \bigstrut\\
\cline{2-7}          & \multicolumn{1}{c|}{FE} & \multicolumn{1}{c||}{DC} & \multicolumn{1}{c|}{FE} & \multicolumn{1}{c||}{DC } & \multicolumn{1}{c|}{FE} & \multicolumn{1}{c|}{DC} \bigstrut\\
    \hline
    Q1    & 5.1\% & 1.6\% & 3.2\% & 2.1\% & 2.2\% & 1.7\% \bigstrut\\
    \hline
    Q2    & 15.0\% & 3.6\% & 14.2\% & 6.5\% & 8.1\% & 4.3\% \bigstrut\\
    \hline
    Q3    & 0.6\% & 0.1\% & 17.3\% & 5.3\% & 8.2\% & 12.7\% \bigstrut\\
    \hline
    Q4    & 0.3\% & 0.1\% & 0.8\% & 0.4\% & 0.5\% & 0.6\% \bigstrut\\
    \hline
    Q5    & 0.2\% & 0.1\% & 1.2\% & 0.5\% & 0.9\% & 0.6\% \bigstrut\\
    \hline
    Q6    & 0.1\% & 0.1\% & 3.7\% & 0.8\% & 5.3\% & 1.7\% \bigstrut\\
    \hline
    \end{tabular}%
\end{table}%

The overhead of estimating frequent vertices and copying their neighbor lists to GPU (i.e., Step-2 and Step-3 in Fig.~\ref{fig:Overview}) are included in the execution time of \texttt{GCSM} in Fig.~\ref{fig:performance_fr} to Fig.~\ref{fig:performance_10k}.  
Details of the breakdown overhead are given in Table~\ref{tab:breakdown}. 
We can see that the overhead of frequency estimation is small, accounting for less than 10\% of the total execution time in most cases. 
The percentage decreases as the pattern size becomes larger.  
The overhead of copying the neighbor lists of frequent vertices to GPU is also small, accounting for less than 5\% of total execution time in most cases. 




The results also indicate that the naive caching policy based on node degree (\texttt{Naive}) is ineffective. Its performance is almost the same as zero-copy (which accesses all data from CPU memory). This is because the access of nodes during the matching procedure depends on the query pattern and updated edges. Having a high node degree does not necessarily result in more frequent access. 

Continuous subgraph matching exhibits data locality not only because most real-world graphs have a skewed degree distribution, but also because the matching is performed on small batches of updated edges. 
The performance results on RoadNetPA and RoadNetCA (where all nodes have a small degree) validate that our system is still efficient when the input graph is less skewed. 
As shown in Fig.~\ref{fig:performance_roadnet}, GCSM achieves 1.6x to 2.0x speedups against the zero-copy implementation and 1.6x to 2.1x speedups against the naive caching policy. 
Note that we tested the performance with all size-3, 4, and 5 motifs instead of specific patterns in this case because the listed patterns in Fig.~\ref{fig:queries} rarely exist in the road nets.



We also test the performance of naive unified memory implementation (\texttt{UM}). 
The execution time is not plotted in the figures because it is much longer than other versions, making the figures out of scale. 
For the test cases in Fig.~\ref{fig:performance_fr} to Fig.~\ref{fig:performance_roadnet}, \texttt{UM} is 69x to 210x slower than the naive zero-copy. 

%\textcolor{red}{We implement the naive caching strategy(\texttt{naive}) that GPU only caches the vertices with the highest degree in the graph. For each batch of edge updates, the GPU cache size limitation for \texttt{naive} is set as the same as the cache size used by \texttt{GCSM}. There is no significant performance gap between \texttt{naive} and \texttt{ZP} which means that the naive caching strategy doesnâ€™t benefit well from the graph skewness since the actual vertices visited during matching overlap slightly with the highest degree vertices in the graph. }

%\textcolor{red}{We also implement another naive caching strategy. For each batch of edge update, GPU caches the highest degree vertices of its 2-hops neighbors. But there is a trade-off. Caching more vertices means better performance but huge data transfer overhead(as shown in Fig. 13), and caching fewer vertices means no speedups. Getting its 2-hops neighbor is also a big overhead. We try to find a balance for this trade-off but the best performance achieved is far worse than \texttt{ZP}.  Due to that, its execution time is not plotted in the figure. }







To compare with \texttt{VSGM}, which copies all $k$-hop neighbors of the updated edges onto  GPU, we have to use smaller batch sizes (128 for SF3K and 64 for SF10K) to fit the data into GPU memory. Figure~\ref{fig:vsgm} shows the breakdown of execution time for processing one batch of edge updates with both \texttt{VSGM} and \texttt{GCSM}. We can see that the subgraph matching kernel takes almost the same amount of time in \texttt{GCSM} as in \texttt{VSGM}, indicating that our system incurs little overhead of CPU data access. 
Although \texttt{VSGM} avoids accessing data from the CPU entirely, it requires copying much more data to the GPU before the matching process. This results in a much longer data copy time and thus much worse overall performance. 


To show the efficiency of our system on different batch sizes, we test the performance with eight batch sizes from 8192 to 64 on SF3K and SF10K. As shown in Fig.~\ref{fig:performance_batchsize}, the execution time is almost proportional to the batch size. Our system achieves 1.8x to 2.9x speedups (with an average of 2.1x) against zero-copy, and 1.6x to 2.8x speedups (with an average of 1.9x) against the naive degree-based cache policy. 




\subsection{Comparison with CPU Implementations}

\input{fig/6.vsgm.tex}
\input{fig/6.Rapidflow}


The execution time of our CPU implementation is included in 
 Fig.~\ref{fig:performance_fr} to Fig.~\ref{fig:performance_10k}.  
The CPU implementation is slower than the naive zero-copy implementation on GPU (for most cases), validating the benefit of GPU processing for subgraph matching. 
Our system achieves speedups ranging from 1.4x to 11.4x, with an average of 4.1x, compared to the CPU implementation. 

The RapidFlow system runs out of CPU memory when storing candidate vertices on the three large graphs. Therefore, we compare the performance with RapidFlow using two smaller graphs (AZ and LJ). 
The results are shown in Fig.\ref{fig:rapidflow}. 
RapidFlow achieves comparable performance to our CPU implementation in most cases, validating the efficiency of our CPU baseline. 
However, due to its optimized matching order, RapidFlow can be up to 7.7x faster than our CPU implementation in some cases. 
This performance advantage of RapidFlow is at the cost of extra memory consumption for storing the candidate vertices for each pattern vertex. Nevertheless, our GPU system outperforms RapidFlow by 1.6x to 4.4x in all cases. 
It will be interesting to reduce the memory consumption of RapidFlow and incorporate its matching order optimization into our system.  
We will leave the problem for future work. 



\subsection{Effectiveness of Frequency Estimation}
\label{sec:coverage}
% \input{table/overlap}
\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{oneside,margin={0.3cm,0cm}}
     \subfloat[Memory Access Distribution]{
    \includegraphics[scale=0.33, page=1]{expfig/percentage-crop.pdf}
    \label{fig:dist}
    }\hfil
    \captionsetup[subfigure]{oneside,margin={0.7cm,0cm}}
    \subfloat[Cache Coverage]{
    \includegraphics[scale=0.33, page=1]{expfig/coverage-crop.pdf}
    \label{fig:coverage}
    }
    \caption{Memory access distribution and cache coverage of incremental matching.}
    \label{fig:dist_cover}
\end{figure}



To show the effectiveness of our random walk technique in identifying frequent vertices, we calculate the coverage of accessed vertices within the GPU cache. 
Suppose $S$ is the set of most frequently accessed vertices during the exact matching process and $T$ is the set of vertices cached on the GPU. The coverage is calculated as $|S\cap T|/|S|$. 


Figure~\ref{fig:coverage} shows the coverage results for different test cases at varying percentages of the most frequent vertices.
For SF3K, the coverages for the top 1\% to top 5\% frequent vertices are nearly 100\%. 
For SF10K, we achieve coverage of more than 90\% for the top 1\% frequent vertices and coverage of about 75\% for the top 5\% frequent vertices.
For FR graph, our method identifies all of the top 1\% frequent vertices and more than 91\% of the top 5\% frequent vertices. 
The coverage decreases as percentage of vertices increases.  However, since more than 80\% memory access is made to the top-5\% vertices (as shown in Fig.~\ref{fig:dist}), the cached data on GPU can save most of the accesses to CPU during the matching process. 


\subsection{Graph Reorganization Overhead}


%\begingroup
%\setlength{\columnsep}{1em}%
%\begin{wrapfigure}{r}{0.19\textwidth}
\begin{table}[t]
\centering
    \footnotesize
        \caption{Graph reorganization time (in milliseconds).}
    \label{tab:update}
    \begin{tabular}{c|c|c}
     & $|\Delta E|=4096$ &  $|\Delta E|=8192$             \\ \hline 
    AZ           & 1.5        & 3.4           \\ \hline
    PA           & 0.8        & 1.5          \\ \hline
    CA           & 0.9        & 1.5           \\ \hline
    LJ       & 3.1        & 8.4           \\ \hline
    FR         & 7.9	        & 8.8            \\ \hline
    SF3K        & 5.8         & 6.4           \\ \hline
    SF10K       & 6.7        & 9.5           \\ \hline
     % \vspace{-1mm}
    \end{tabular}
\end{table}
So far we have evaluated Step-1 to Step-4 of our system shown in Fig.~\ref{fig:Overview}. 
The last step of our system is to reorganize the graph data on CPU. 
It involves removing the deleted edges and sorting all the updated neighbor lists. While our system is not designed to achieve the best performance for edge insertion and deletion, We find the overhead of graph update is almost negligible compared to the matching process. As shown in Table~\ref{tab:update}, the average time of graph reorganization for a batch of edge updates is no more than a few milliseconds, which is much smaller than the matching time in Fig.~\ref{fig:performance_fr} to Fig.~\ref{fig:performance_roadnet}. 
The graph reorganization overhead reduces as the batch size decreases, and it is always negligible compared to the matching time for batch sizes from 64 to 8192. 




