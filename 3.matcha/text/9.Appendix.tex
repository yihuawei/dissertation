\clearpage
\section{Appendix}


%------------------------------------------------------------------------

\subsection{Subgraph Matching Algorithms Written in Matcha}
\label{apx:examples}
\noindent
\textbf{\textit{Backtracking With Vertex Extension.}}
Listing~\ref{fig:gminer_code} shows the Matcha code for re-implementing G2Miner on the query shown in Figure~\ref{fig:gminer_query}. The pseudo-code of the nested for-loop for matching the query is shown as the comment. $E$ is an array of all the edges in the data graph G. $\cap$ is set intersection and $-$ is set difference. If $u_k$ in the query is connected to $u_i$ and $u_j$, the candidate set of $v_k$ is computed by doing set intersection between the neighbor sets of $v_i$ and $v_j$. For example, the candidate set $C2$ of $v_2$ is computed by $N(v_0) \cap N(v_1)$ since $u_2$ is connected to both $u_0$ and $u_1$ in the query graph. In the innermost loop, $v_{0}v_{1}v_{2}v_{3}$ is a valide matched subgraph in $G$.


In the Matcha implementation, the algorithm starts by iterating over all the graph edges (Line 25) on an \texttt{apply} node. Then it computes candidate set $C2$ by performing set intersection (Line 19) on the neighbor sets of $v_0$ and $v_1$, which are the endpoints of $e$ and can be obtained through the functions \texttt{get(e,0)} and \texttt{get(e,1)}. The candidate set $C3$ is $N(v_2) - \{v0, v1\}$, which removes vertices in $N(v_2)$ that are duplicated with $v_0$ and $v1$. Finally, the \texttt{apply} function (Line 25) returns an array recording the partial counting results of each edge $e$, and the \texttt{sum} function accumulates these partial results to get the final result. 

Matcha also supports labeled matching. The query nodes in Figure~\ref{fig:gminer_query} are labeled with labels $L0L1L2L3$. For labeled queries, we can pass a filtering function as the cond parameter when building \texttt{apply} node, then the loop generated by the \texttt{apply} node will have an $if$ condition to filter out vertices with mismatched labels. For example, we can build a label filtering function having return statements like $vertex\_label(G, v0)==L0$. The more specific usage example of conditional \texttt{apply} can be found in the part "Temporal Subgraph Matching".



\begin{figure}[ht]
    \centering
\lstinputlisting[language=Python, mathescape=true, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, 
numbers=left,
tabsize=4]{codelistings/gminer.py}
\captionof{lstlisting}{Re-implementation of G2Miner in Matcha on the query in Figure~\ref{fig:gminer_query}.}
\label{fig:gminer_code}
\end{figure}

\begin{figure}
    \subfloat[Query for GM]{
        \includegraphics[scale=0.45, page=4]{fig/example1-crop.pdf}
        \label{fig:gminer_query}
    }\hfil
    \subfloat[Query for DM]{
        \includegraphics[scale=0.45, page=5]{fig/example1-crop.pdf}
        \label{fig:decomine_query}
    }\hfil 
    \subfloat[Query for EV]{
        \includegraphics[scale=0.45, page=6]{fig/example1-crop.pdf}
        \label{fig:everest_query}
    }\hfil
    \caption{Queries used for re-implementing G2Miner (GM), Decomine (DC), and Everest (EV)}
    \label{fig:apx_queries}
\end{figure}

% The matching procedure is very similar to Decomine, except that it operates on a labeled query and does not involve pattern decomposition. . 
\noindent
\textbf{\textit{Pattern Decomposition.}}
Listing~\ref{fig:decomine_code} shows the code written in Matcha to  re-implement Decomine for matching the query in Figure~\ref{fig:decomine_query}. The comment in Listing~\ref{fig:decomine_code} gives the the pseudo-code of Decomine. The loop over $E$ is generated by the \texttt{apply} node, and all the statements in the loop body have corresponding ones in the Matcha code. 

The query edge $u_{0}u_{1}$ can be mapped to any edge in the data graph, so the algorithm starts by iterating over all the edges (Line 21). Then it computes candidate set C2 by performing set intersection (Line 16) on the neighbor of $v_0$ and $v_1$, which are the endpoints of $e$ and can be obtained through the functions \texttt{get(e,0)} and \texttt{get(e,1)}. Then we can get the matching result on the current edge $e$ by a formulation $tri * (u + v)$ since the query is decomposable because $u_3$ and $u_2$ are isolated by the edge $u_{0}u_{1}$. Finally, the \texttt{sum} function (Line 21) accumulates the partial results of each edge $e$ returned by \texttt{apply} function to get the final result.

% Finally, the \texttt{apply} function (Line 21) returns an array recording the partial counting results of each edge $e$, and the \texttt{sum} function accumulates these partial results to get the final result. 



\noindent
\textbf{\textit{Temporal Subgraph Matching.}}
Listing~\ref{fig:everest_code} provides an example code for implementing Everest to perform temporal subgraph matching on the temporal query shown in Figure~\ref{fig:everest_query}. We also include the pseudo-code of Everest as comments at the top of the Matcha code.

Unlike Decomine and G2Miner, which backtrack vertex-by-vertex, Everest backtracks edge-by-edge to match a query edge at each step. Similar to Decomine, Everest starts by iterating over all the graph edges (Line 49). It then obtains the candidate set $C2$ (Line 29) by visiting $v1$'s neighbors; a similar procedure is applied to obtain $C3$. There are three nested \texttt{apply} statements iterating over the candidate set of each query edge. Each \texttt{apply} returns an array, with each element recording the partial result from a matched subgraph. For example, \texttt{apply} on $edges$ returns an array, and each element is the partial result from a matched edge. The \texttt{sum} function sums all the partial results for each array returned by \texttt{apply}, and the outermost \texttt{sum} returns the final result. 




%------------------------------------------------------------------------

\subsection{IR Generation Rules}
\label{apx:irrule}

\begin{table}[t]
\footnotesize
\centering
\caption{IR generation rule.}
\label{tab:irrule}
\begin{tabular}{p{2.82cm}|p{4.9cm}}
\multicolumn{1}{c|}{\textbf{ASG Node}} & \multicolumn{1}{c}{\textbf{IR}}                                                                   \\ \hline\hline
mk\_var(dtype, val)                                                                           & \begin{tabular}[c]{@{}l@{}}\textbf{out}: Scalar(dtype)\\  \textbf{inst}: Assign(out, val) \textit{if val is provided}\end{tabular}                  \\ \hline
arithmetic, logical, and comparison operations                                       & \textbf{out}: Expr(op, var1.out, var2.out)    \\
\hline\hline
mk\_tuple(nvars, dtype, val)                                          & \begin{tabular}[c]{@{}l@{}}\textbf{out}: Array(dtype, (nvars, ))\\ \textbf{inst}: \textit{Assigns val to out if val is provided}\end{tabular}                                                  \\ \hline
get(t, pos)                                                                                   & \textbf{out}: Index(t.out, pos.out)       \\  \hline\hline
mk\_graph(edges, vlabel, einfo)                                                 & \begin{tabular}[c]{@{}l@{}}\textbf{aux}: nbr = Array(int, (edges.nitems, )), \\ eInfo = Array(einfo.dtype, (edges.nitems, )), \\ vLabel = vlabel, indPtr = Array(int, (…))\\ \textbf{inst}: \_init\_graph(...)\end{tabular}              \\ \hline
neighbor/edge\_info(g, v)                     & \begin{tabular}[c]{@{}l@{}}\textbf{aux}: 
h = Scalar(int), \\ p0 = Scalar(int),  p1 = Scalar(int)\\ \textbf{out}: Index(g.nbr/g.eInfo, p0:p1)\\ \textbf{inst}: \textit{If g is in CSR format}: \\ Assign(h, v.out), \\ Assign(p0, Index(g.indPtr, h))\\ Assign(p1, Index(g.indPtr, h+1)); \\ \textit{else if g.indPtr is a hash table}: \\  Assign(h, \_hash(v.out)), \\Assign(p0, Index(g.indPtr, h, 1)), \\ Assign(p1, Index(g.indPtr, h, 2))\end{tabular}                           \\ \hline
vertex\_label(g, v)                           & \begin{tabular}[c]{@{}l@{}}\textbf{aux}: h = Scalar(int)\\ \textbf{out}: Scalar(int)\\ \textbf{inst}: Assign(h, v.out) \textit{if g is in CSR format}  \\ \textit{else} Assign(h, \_hash(v.out)),
 \\ Assign(out, Index(g.vLabel, h, 1))\end{tabular} \\ \hline\hline

mk\_list(nitems, type, val)                                                                       & \begin{tabular}[c]{@{}l@{}}\textbf{out}: Array(type.dtype, (nitems, *type.dim))\\ \textbf{inst}: \textit{Assigns val to out if val is provided}\end{tabular}                                                               \\ \hline 
apply(func, cond, l1, …) & \begin{tabular}[c]{@{}l@{}}\textit{If cond is None}: \\ \textbf{out}: Array(func.out.dtype, \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(l1.nitems, *func.out.dim))\\ \textbf{inst}: Loop(0, l1.nitems, {[}\\ \;\;\;\;\;\;\;\;\;\;\;\;inst on all descendants, \\\;\;\;\;\;\;\;\;\;\;\;\;Assign(Index(out, i), func.out){]})\\ \textit{else:} \\
\textbf{aux}: pos = Scalar(int)\\
\textbf{out}: Index(Array(func.out.dtype, \\\;\;\;\;\;\;\;\;\;\;\;(l1.nitems, *func.out.dim)), 0:pos)\\ \textbf{inst}: Assign(pos, 0), \\          CondLoop(0, data.nitems, cond.out, {[}\\\;\;\;\;\;\;\;\;\;\;\;\;inst on all descendants,\\\;\;\;\;\;\;\;\;\;\;\;\;Assign(Index(out, pos), func.out), \\\;\;\;\;\;\;\;\;\;\;\;\;Assign(pos, Expr(+, pos, 1)){]}{)}\end{tabular}\\ \hline
intersect/diff(l1, l2)                        & \begin{tabular}[c]{@{}l@{}} \textbf{aux}: pos = Scalar(int)\\ \textbf{out}: Index(Array(l1.dtype, \\\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(l1.nitems, )), 0:pos)\\\textbf{inst}: \_intersect/\_diff(l1.out, l2.out, pos)\end{tabular}                            \\ \hline
sum(l)                            & \begin{tabular}[c]{@{}l@{}}\textbf{out}: Scalar(l.dtype)\\ \textbf{inst}: Assign(out, 0), Loop(0, l.nitems, {[}\\\;\;Assign(out, Expr(+, out, Index(l.out, i))){]})\end{tabular}                \\ \hline
\end{tabular}
\end{table}

Table~\ref{tab:irrule} summarizes the IR generation rules for all Matcha operators. The output of \texttt{mk\_var} is a \texttt{Scalar}. If an initial value is provided, an \texttt{Assign} is added to the instruction list ($inst$) to initialize the scalar variable. 
The output of \texttt{mk\_tuple} and \texttt{mk\_list} is an \texttt{Array}. The size of the array is the number of items concatenated by the size of each item. 
The output of \texttt{get} is simply an indexing to the Array of the input Tuple. 
The output of an arithmetic, comparison, or logical operation is an \texttt{Expr} on the outputs of its operands. 

To represent the iteration of a \texttt{List}, the IR has a \texttt{Loop(lb, up, body)} construct where $lb, up$ are the lower bound and upper bound of the loop iterator (with the step size always being 1), and the loop body is a list of IR instructions. A similar construct is \texttt{CondLoop(lb, up, cond, body)}, where the $cond$ is an \texttt{Expr} indicating the condition of whether the loop body should be executed in each iteration. 
The two constructs are used to implement the apply operator. When $cond$ is not provided, the output of \texttt{apply} has the number of items as the input \texttt{List}. 
Thus, the output is an \texttt{Array} that has the same size as the input in the first dimension. The size of the remaining dimensions is determined by the return data type of $func$. 
The instruction for computing the output is a \texttt{Loop} that iterates from 0 to $l1.nitems$. The loop's body contains all the $inst$s of the subtree rooted at the \texttt{apply} node. 
When $cond$ is provided, the output of \texttt{apply} usually has fewer items than the input \texttt{List}. Because the number is unknown at compile-time, we preserve an \texttt{Array} of the same size as input and use an auxiliary variable ($pos$) to keep track of the number of items at runtime. The output is represented as a slicing of the \texttt{Array}. 
In the body of \texttt{CondLoop}, an Assign is appended at the end to increase $pos$ by 1. 

The \texttt{intersect/diff} operator is conducted on two \texttt{List}s of \texttt{Var}s of the same data type. Depending on whether the input \texttt{List}s are sorted, different algorithms can be used. Since the two operations are well studied and there are plenty of efficient implementations on different platforms, instead of generating \texttt{Loop/CondLoop}s, we simply call a built-in function to perform the operation. 
After the function call, we store the output size in a variable ($pos$) so we can represent the output as a slicing of \texttt{Array}. 

%------------------------------------------------------------------------

\subsection{Optimized Code}
\label{apx:opt}
Figure~\ref{fig:opt_code} shows the steps to optimize the code shown in Figure~\ref{fig:ccode}.

\noindent
\textbf{\textit{Operator Fusion.}}
As shown in Figure~\ref{fig:opt_code0}, two loops, originally marked by grey and purple in Figure~\ref{fig:ccode}, are fused by the fusion rule between \texttt{sum} and \texttt{apply} node. 

\noindent
\textbf{\textit{Code Motion.}}
After code motion, statements like $\_init\_graph$ are moved out of the $i0$ loop, and some other statements marked in orange and green are moved out of the $i1$ loop. A statement can be moved out of a loop if itself and all the statements it depends on do not depend on the loop iterator. For example, $\_init\_graph$ does not depend on the iterator $i0$, and $lst[1] = L01[i0][1]$ does not depend on the iterator $i1$.

\noindent
\textbf{\textit{CPU Parallelization.}}
In CPU parallelization, we first extend dimensions by one for variables to be updated like $nb\_h$ and $sum1$ in the loop, and then move them into the stack registers after memory optimization, so we can see $nb\_h$ is finally declared as a local $int$ variable. Since $sum1$ is the final output of $apply1$, it will not be influenced by memory optimization. Then we attach the \texttt{pragma omp parallel...} to the loops to be parallelized and insert a variable $tid$ in the loop body. Figure~\ref{fig:opt_code2} shows the CPU code after parallelization.

\noindent
\textbf{\textit{GPU Parallelization.}}
As shown in Figure~\ref{fig:opt_code3}, to generate the GPU code, the nested for-loop will be extracted to form a GPU kernel function, which will be called in the CPU function body. The memory allocation statements for those variables used on GPU will also be replaced by $cudaMalloc$. The outermost loop is executed in parallel by warps.

%------------------------------------------------------------------------

% \clearpage

\begin{figure}[t]
    \centering
\lstinputlisting[language=Python, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, 
numbers=left,
tabsize=4]{codelistings/decomine.py}
\captionof{lstlisting}{Re-implementation of DecoMine in Matcha on the query in Figure~\ref{fig:decomine_query}.}
\label{fig:decomine_code}
\end{figure}

\begin{figure}[t]
    \centering
\lstinputlisting[language=Python, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, 
numbers=left,
tabsize=4]{codelistings/everest.py}
\captionof{lstlisting}{Re-implementation of Everest in Matcha on the query in Figure~\ref{fig:everest_query}.}
\label{fig:everest_code}
\end{figure}



%------------------------------------------------------------------------

\clearpage
\newpage

% \begin{figure}[ht]
% \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt0.cpp}
% \label{fig:ccode}
%     \caption{(1) Generated C++ code after Fusion.}
%     \label{fig:opt_code}
% \end{figure}

% \begin{figure}[ht]
% \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt1.cpp}
% \label{fig:ccode}
%     \caption{(2) Generated C++ code after code motion.}
%     \label{fig:opt_code}
% \end{figure}

% \begin{figure}[ht]
% \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt2.cpp}
% \label{fig:ccode}
%     \caption{(3.1) Generated C++ code after CPU Parallelization.}
%     \label{fig:opt_code}
% \end{figure}

% \begin{figure}[ht]
% \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt3.cpp}
% \label{fig:ccode}
%     \caption{(3.2) Generated C++ after GPU Parallelization.}
%     \label{fig:opt_code}
% \end{figure}


\begin{figure*}[t]
    \centering
    \subfloat[Generated C++ code after operator fusion.]{ 
    \begin{minipage}{8.5cm}
        \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=fa, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt0.cpp}
        \label{fig:opt_code0}    
        \end{minipage}}\hfil
    \subfloat[Generated C++ code after code motion.]{  \begin{minipage}{8.5cm}
        \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt1.cpp}
        \label{fig:opt_code1} \end{minipage}
    }  \\ \vspace{1em}
    \subfloat[Generated C++ code after CPU parallelization.]{\begin{minipage}{8.5cm}
        \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt2.cpp}
        \label{fig:opt_code2}\end{minipage}
    }    
    \hfil
    \subfloat[Generated C++ code after GPU parallelization.]{ \begin{minipage}{8.5cm}
        \lstinputlisting[language=C++, keywordstyle=\color{blue}, stringstyle=\color{red},  breaklines=true, showspaces=false, showstringspaces=false, tabsize=4]{codelistings/opt3.cpp}
        \label{fig:opt_code3}\end{minipage}
    }  \\
    \caption{The Steps to optimize the code shown in Figure~\ref{fig:ccode}.}
    \label{fig:opt_code}
\end{figure*}